{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "library2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOgx3wkacYtmsZ5K3HVz1QK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/contextrcnn2/Context-R-CNN/blob/main/library2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc8ka1mvPLaF"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "_PADDING_VALUE = -100000\n",
        "\n",
        "class FreezableBatchNorm(tf.keras.layers.BatchNormalization):\n",
        "  def __init__(self, training=None, **kwargs):\n",
        "    super(FreezableBatchNorm, self).__init__(**kwargs)\n",
        "    self._training = training\n",
        "\n",
        "  def call(self, inputs, training=None):\n",
        "    if self._training is False: \n",
        "      training = self._training\n",
        "    return super(FreezableBatchNorm, self).call(inputs, training=training)\n",
        "\n",
        "class ContextProjection(tf.keras.layers.Layer):\n",
        "  def __init__(self, projection_dimension, **kwargs):\n",
        "    self.batch_norm = FreezableBatchNorm(\n",
        "        epsilon=0.001,\n",
        "        center=True,\n",
        "        scale=True,\n",
        "        momentum=0.97,\n",
        "        trainable=True)\n",
        "    self.projection = tf.keras.layers.Dense(units=projection_dimension,\n",
        "                                            use_bias=True)\n",
        "    self.projection_dimension = projection_dimension\n",
        "    super(ContextProjection, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.projection.build(input_shape)\n",
        "    self.batch_norm.build(input_shape[:1] + [self.projection_dimension])\n",
        "\n",
        "  def call(self, input_features, is_training=False):\n",
        "    return tf.nn.relu6(self.batch_norm(self.projection(input_features),\n",
        "                                       is_training))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOzs19l6PwXg"
      },
      "source": [
        "class AttentionBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, bottleneck_dimension, attention_temperature,\n",
        "               output_dimension=None, is_training=False,\n",
        "               name='AttentionBlock', max_num_proposals=100,**kwargs):\n",
        "    \n",
        "    self._key_proj = ContextProjection(bottleneck_dimension)\n",
        "    self._val_proj = ContextProjection(bottleneck_dimension)\n",
        "    self._query_proj = ContextProjection(bottleneck_dimension)\n",
        "    self._feature_proj = None\n",
        "    self._attention_temperature = attention_temperature\n",
        "    self._bottleneck_dimension = bottleneck_dimension\n",
        "    self._is_training = is_training\n",
        "    self._output_dimension = output_dimension\n",
        "    self._max_num_proposals = max_num_proposals\n",
        "    if self._output_dimension:\n",
        "      self._feature_proj = ContextProjection(self._output_dimension)\n",
        "    super(AttentionBlock, self).__init__(name=name, **kwargs)\n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    \n",
        "    if not self._feature_proj:\n",
        "      self._output_dimension = input_shapes[-1]\n",
        "      self._feature_proj = ContextProjection(self._output_dimension)\n",
        "\n",
        "  def call(self, box_features, context_features, valid_context_size,\n",
        "           num_proposals):\n",
        "   \n",
        "    _, context_size, _ = context_features.shape\n",
        "    keys_values_valid_mask = compute_valid_mask(\n",
        "        valid_context_size, context_size)\n",
        "\n",
        "    total_proposals, height, width, channels = box_features.shape\n",
        "    batch_size = total_proposals // self._max_num_proposals\n",
        "    box_features = tf.reshape(\n",
        "        box_features,\n",
        "        [batch_size,\n",
        "         self._max_num_proposals,\n",
        "         height,\n",
        "         width,\n",
        "         channels])\n",
        "\n",
        "    box_features = tf.reduce_mean(box_features, [2, 3])\n",
        "\n",
        "    queries_valid_mask = compute_valid_mask(num_proposals,\n",
        "                                            box_features.shape[1])\n",
        "    queries = project_features(\n",
        "        box_features, self._bottleneck_dimension, self._is_training,\n",
        "        self._query_proj, normalize=True)\n",
        "    keys = project_features(\n",
        "        context_features, self._bottleneck_dimension, self._is_training,\n",
        "        self._key_proj, normalize=True)\n",
        "    values = project_features(\n",
        "        context_features, self._bottleneck_dimension, self._is_training,\n",
        "        self._val_proj, normalize=True)\n",
        "\n",
        "    keys *= tf.cast(keys_values_valid_mask[..., tf.newaxis], keys.dtype)\n",
        "    queries *= tf.cast(queries_valid_mask[..., tf.newaxis], queries.dtype)\n",
        "\n",
        "    weights = tf.matmul(queries, keys, transpose_b=True)\n",
        "    weights, values = filter_weight_value(weights, values,\n",
        "                                          keys_values_valid_mask)\n",
        "    weights = tf.nn.softmax(weights / self._attention_temperature)\n",
        "\n",
        "    features = tf.matmul(weights, values)\n",
        "    output_features = project_features(\n",
        "        features, self._output_dimension, self._is_training,\n",
        "        self._feature_proj, normalize=False)\n",
        "    output_features = output_features[:, :, tf.newaxis, tf.newaxis, :]\n",
        "    return output_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmCBN6l7Rrnz"
      },
      "source": [
        "def filter_weight_value(weights, values, valid_mask):\n",
        "  w_batch_size, _, w_context_size = weights.shape\n",
        "  v_batch_size, v_context_size, _ = values.shape\n",
        "  m_batch_size, m_context_size = valid_mask.shape\n",
        "  if w_batch_size != v_batch_size or v_batch_size != m_batch_size:\n",
        "    raise ValueError('Please make sure the first dimension of the input'\n",
        "                     ' tensors are the same.')\n",
        "\n",
        "  if w_context_size != v_context_size:\n",
        "    raise ValueError('Please make sure the third dimension of weights matches'\n",
        "                     ' the second dimension of values.')\n",
        "\n",
        "  if w_context_size != m_context_size:\n",
        "    raise ValueError('Please make sure the third dimension of the weights'\n",
        "                     ' matches the second dimension of the valid_mask.')\n",
        "\n",
        "  valid_mask = valid_mask[..., tf.newaxis]\n",
        "  weights += tf.transpose(\n",
        "      tf.cast(tf.math.logical_not(valid_mask), weights.dtype) *\n",
        "      _PADDING_VALUE,\n",
        "      perm=[0, 2, 1])\n",
        "  values *= tf.cast(valid_mask, values.dtype)\n",
        "\n",
        "  return weights, values\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmwTvfOGSwFO"
      },
      "source": [
        "def project_features(features, bottleneck_dimension, is_training,\n",
        "                     layer, normalize=True):\n",
        "  shape_arr = features.shape\n",
        "  batch_size, _, num_features = shape_arr\n",
        "  features = tf.reshape(features, [-1, num_features])\n",
        "\n",
        "  projected_features = layer(features, is_training)\n",
        "\n",
        "  projected_features = tf.reshape(projected_features,\n",
        "                                  [batch_size, -1, bottleneck_dimension])\n",
        "\n",
        "  if normalize:\n",
        "    projected_features = tf.keras.backend.l2_normalize(projected_features,\n",
        "                                                       axis=-1)\n",
        "\n",
        "  return projected_features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFgutQZ8S26q"
      },
      "source": [
        "def compute_valid_mask(num_valid_elements, num_elements):\n",
        "  batch_size = num_valid_elements.shape[0]\n",
        "  element_idxs = tf.range(num_elements, dtype=tf.int32)\n",
        "  batch_element_idxs = tf.tile(element_idxs[tf.newaxis, ...], [batch_size, 1])\n",
        "  num_valid_elements = num_valid_elements[..., tf.newaxis]\n",
        "  valid_mask = tf.less(batch_element_idxs, num_valid_elements)\n",
        "  return valid_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwL2-IsyTFoM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}