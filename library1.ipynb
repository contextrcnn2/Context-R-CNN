{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "library1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMysEjYHBRpXmelqoevIPC7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/contextrcnn2/Context-R-CNN/blob/main/library1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73DC5NnZJC4X"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import tensorflow.compat.v1 as tf\n",
        "!pip install tf_slim\n",
        "import tf_slim as slim\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5GgE5rRJWF1"
      },
      "source": [
        "_PADDING_VALUE = -100000\n",
        "\n",
        "def filter_weight_value(weights, values, valid_mask):\n",
        "  v_batch_size, v_context_size, _ = values.shape \n",
        "  w_batch_size, _, w_context_size = weights.shape\n",
        "  m_batch_size, m_context_size = valid_mask.shape\n",
        "  if v_batch_size != m_batch_size or w_batch_size != v_batch_size:\n",
        "    raise ValueError(\"please make the first dimensions same\")\n",
        "\n",
        "  if w_context_size != v_context_size:\n",
        "    raise ValueError(\"Please make the third dimension of weights same as\"\n",
        "                     \" the second dimension of values.\")\n",
        "  if w_context_size != m_context_size:\n",
        "    raise ValueError(\"Please make sure the third dimension of the weights\"\n",
        "                     \" matches the second dimension of the valid_mask.\")\n",
        "  valid_mask = valid_mask[..., tf.newaxis]\n",
        "  very_negative_mask = tf.ones(\n",
        "      weights.shape, dtype=weights.dtype) *_PADDING_VALUE\n",
        "  valid_weight_mask = tf.tile(tf.transpose(valid_mask, perm=[0, 2, 1]),\n",
        "                              [1, weights.shape[1], 1])\n",
        "  weights = tf.where(valid_weight_mask,\n",
        "                     x=weights, y=very_negative_mask)\n",
        "\n",
        "  values *= tf.cast(valid_mask, values.dtype)\n",
        "\n",
        "  return weights, values\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rA_HdzPMEq1"
      },
      "source": [
        "def compute_valid_mask(num_valid_elements, num_elements):\n",
        "  batch_size = num_valid_elements.shape[0]\n",
        "  element_idxs = tf.range(num_elements, dtype=tf.int32)\n",
        "  batch_element_idxs = tf.tile(element_idxs[tf.newaxis, ...], [batch_size, 1])\n",
        "  num_valid_elements = num_valid_elements[..., tf.newaxis]\n",
        "  valid_mask = tf.less(batch_element_idxs, num_valid_elements)\n",
        "  return valid_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW16TeP5MRvI"
      },
      "source": [
        "def project_features(features, projection_dimension, is_training, normalize):\n",
        "  batch_norm_params = {\n",
        "      \"is_training\": is_training,\n",
        "      \"epsilon\": 0.001,\n",
        "      \"decay\": 0.97,\n",
        "      \"center\": True,\n",
        "      \"scale\": True}\n",
        "  batch_size, _, num_features = features.shape\n",
        "  features = tf.reshape(features, [-1, num_features])\n",
        "  projected_features = slim.fully_connected(\n",
        "      features,\n",
        "      num_outputs=projection_dimension,\n",
        "      activation_fn=tf.nn.relu6,\n",
        "      normalizer_fn=slim.batch_norm,\n",
        "      normalizer_params=batch_norm_params)\n",
        "\n",
        "  projected_features = tf.reshape(projected_features,\n",
        "                                  [batch_size, -1, projection_dimension])\n",
        "  if normalize:\n",
        "    projected_features = tf.math.l2_normalize(projected_features, axis=-1)\n",
        "  return projected_features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNuJGY2pM6Pe"
      },
      "source": [
        "def attention_block(input_features, context_features, bottleneck_dimension,\n",
        "                    output_dimension, attention_temperature,\n",
        "                    keys_values_valid_mask, queries_valid_mask,\n",
        "                    is_training, block_name=\"AttentionBlock\"):\n",
        " with tf.variable_scope(block_name):\n",
        "    queries = project_features(\n",
        "        input_features, bottleneck_dimension, is_training, normalize=True)\n",
        "    keys = project_features(\n",
        "        context_features, bottleneck_dimension, is_training, normalize=True)\n",
        "    values = project_features(\n",
        "        context_features, bottleneck_dimension, is_training, normalize=True)\n",
        "    queries *= tf.cast(queries_valid_mask[..., tf.newaxis], queries.dtype)\n",
        "    keys *= tf.cast(keys_values_valid_mask[..., tf.newaxis], keys.dtype)\n",
        "    weights = tf.matmul(queries, keys, transpose_b=True)\n",
        "    weights, values = filter_weight_value(weights, values,\n",
        "                                          keys_values_valid_mask)\n",
        "    weights = tf.identity(tf.nn.softmax(weights / attention_temperature),\n",
        "                          name=block_name+\"AttentionWeights\")\n",
        "    features = tf.matmul(weights, values)\n",
        "\n",
        "    output_features = project_features(\n",
        "      features, output_dimension, is_training, normalize=False)\n",
        "    return output_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoIzfMTzNuQu"
      },
      "source": [
        "def _compute_box_context_attention(box_features, num_proposals,\n",
        "                                   context_features, valid_context_size,\n",
        "                                   bottleneck_dimension,\n",
        "                                   attention_temperature, is_training,\n",
        "                                   max_num_proposals,\n",
        "                                   use_self_attention=False,\n",
        "                                   use_long_term_attention=True,\n",
        "                                   self_attention_in_sequence=False,\n",
        "                                   num_attention_heads=1,\n",
        "                                   num_attention_layers=1):\n",
        "  _, context_size, _ = context_features.shape\n",
        "  context_valid_mask = compute_valid_mask(valid_context_size, context_size)\n",
        "\n",
        "  total_proposals, height, width, channels = box_features.shape\n",
        "\n",
        "  batch_size = total_proposals // max_num_proposals\n",
        "  box_features = tf.reshape(\n",
        "      box_features,\n",
        "      [batch_size,\n",
        "       max_num_proposals,\n",
        "       height,\n",
        "       width,\n",
        "       channels])\n",
        "  box_features = tf.reduce_mean(box_features, [2, 3])\n",
        "  box_valid_mask = compute_valid_mask(\n",
        "      num_proposals,\n",
        "      box_features.shape[1])\n",
        "\n",
        "  if use_self_attention:\n",
        "    self_attention_box_features = attention_block(\n",
        "        box_features, box_features, bottleneck_dimension, channels.value,\n",
        "        attention_temperature, keys_values_valid_mask=box_valid_mask,\n",
        "        queries_valid_mask=box_valid_mask, is_training=is_training,\n",
        "        block_name=\"SelfAttentionBlock\")\n",
        "\n",
        "  if use_long_term_attention:\n",
        "    if use_self_attention and self_attention_in_sequence:\n",
        "      input_features = tf.add(self_attention_box_features, box_features)\n",
        "      input_features = tf.divide(input_features, 2)\n",
        "    else:\n",
        "      input_features = box_features\n",
        "    original_input_features = input_features\n",
        "    for jdx in range(num_attention_layers):\n",
        "      layer_features = tf.zeros_like(input_features)\n",
        "      for idx in range(num_attention_heads):\n",
        "        block_name = \"AttentionBlock\" + str(idx) + \"_AttentionLayer\" +str(jdx)\n",
        "        attention_features = attention_block(\n",
        "            input_features,\n",
        "            context_features,\n",
        "            bottleneck_dimension,\n",
        "            channels.value,\n",
        "            attention_temperature,\n",
        "            keys_values_valid_mask=context_valid_mask,\n",
        "            queries_valid_mask=box_valid_mask,\n",
        "            is_training=is_training,\n",
        "            block_name=block_name)\n",
        "        layer_features = tf.add(layer_features, attention_features)\n",
        "      layer_features = tf.divide(layer_features, num_attention_heads)\n",
        "      input_features = tf.add(input_features, layer_features)\n",
        "    output_features = tf.add(input_features, original_input_features)\n",
        "    if not self_attention_in_sequence and use_self_attention:\n",
        "      output_features = tf.add(self_attention_box_features, output_features)\n",
        "  elif use_self_attention:\n",
        "    output_features = self_attention_box_features\n",
        "  else:\n",
        "    output_features = tf.zeros(self_attention_box_features.shape)\n",
        "  output_features = output_features[:, :, tf.newaxis, tf.newaxis, :]\n",
        "  return output_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7Nj5K3ROQR8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}